{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, tune, and deploy a custom ML model using bias detection & mitigation algorithm in text data from AWS Marketplace\n",
    "\n",
    "\n",
    "The solution uses a Double - Hard DeBias Algorithm to remove targeted biases from the vector space representation of a text corpus.\n",
    "\n",
    "\n",
    "\n",
    "This sample notebook shows you how to Train, tune, and deploy a custom ML model using bias detection & mitigation algorithm in text data from AWS Marketplace\n",
    "\n",
    "\n",
    "> **Note**: This is a reference notebook and it cannot run unless you make changes suggested in the notebook.\n",
    "\n",
    "#### Pre-requisites:\n",
    "1. **Note**: This notebook contains elements which render correctly in Jupyter interface. Open this notebook from an Amazon SageMaker Notebook Instance or Amazon SageMaker Studio.\n",
    "1. Ensure that IAM role used has **AmazonSageMakerFullAccess**\n",
    "1. Some hands-on experience using [Amazon SageMaker](https://aws.amazon.com/sagemaker/).\n",
    "1. To use this algorithm successfully, ensure that:\n",
    "    1. Either your IAM role has these three permissions and you have authority to make AWS Marketplace subscriptions in the AWS account used: \n",
    "        1. **aws-marketplace:ViewSubscriptions**\n",
    "        1. **aws-marketplace:Unsubscribe**\n",
    "        1. **aws-marketplace:Subscribe**  \n",
    "    2. or your AWS account has a subscription to For Seller to update: Airline Crew Pairing Optimization. \n",
    "\n",
    "#### Contents:\n",
    "1. [Subscribe to the algorithm](#1.-Subscribe-to-the-algorithm)\n",
    "1. [Prepare dataset](#2.-Prepare-dataset)\n",
    "\t1. [Dataset format expected by the algorithm](#A.-Dataset-format-expected-by-the-algorithm)\n",
    "\t1. [Configure dataset](#B.-Configure-dataset)\n",
    "\t1. [Upload datasets to Amazon S3](#C.-Upload-datasets-to-Amazon-S3)\n",
    "1. [Execute DeBias model](#3.-Execute-DeBias-Model)\n",
    "\t1. [Set up environment](#3.1-Set-up-environment)\n",
    "\t1. [Train Model](#3.2-Train-Model)\n",
    "    1. [Inspect Output](#3.3-Inspect-the-Output-in-S3)\n",
    "1. [Clean-up](#4.-Clean-up)\n",
    "\t1. [Unsubscribe to the listing (optional)](#Unsubscribe-to-the-listing-(optional))\n",
    "\n",
    "\n",
    "#### Usage instructions\n",
    "You can run this notebook one cell at a time (By using Shift+Enter for running a cell)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Subscribe to the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To subscribe to the algorithm:\n",
    "1. Open the algorithm listing page Airline Crew Pairing Optimization\n",
    "1. On the AWS Marketplace listing,  click on **Continue to subscribe** button.\n",
    "1. On the **Subscribe to this software** page, review and click on **\"Accept Offer\"** if you agree with EULA, pricing, and support terms. \n",
    "1. Once you click on **Continue to configuration button** and then choose a **region**, you will see a **Product Arn**. This is the algorithm ARN that you need to specify while training a custom ML model. Copy the ARN corresponding to your region and specify the same in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_arn ='arn:aws:sagemaker:us-east-2:786796469737:algorithm/double-hard-debias-copy-07-07-copy-07-11'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import json \n",
    "import uuid\n",
    "from sagemaker import ModelPackage\n",
    "import sagemaker as sage\n",
    "from sagemaker import get_execution_role\n",
    "from urllib.parse import urlparse\n",
    "import io\n",
    "import boto3\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import tarfile\n",
    "from zipfile import ZipFile\n",
    "import pandas as pd\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Dataset format expected by the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm requires data in the format as described for best results:\n",
    "* Input File name should be input_data.zip\n",
    "* Within the zip file, inputs must be unstructured text corpus,two text files, each containing words pertaining to specific bias class (Eg: Male word file containing male specific key words), a json with bias specific keywords and a definitional pair json with keywords part of same group but opposite category\n",
    "* The input data files must contain all columns specified in input data description; other columns will be ignored.\n",
    "* For detailed instructions, please refer sample notebook and algorithm input details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Configure dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset='Input/input_data.zip'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C. Upload datasets to Amazon S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sage.Session()\n",
    "bucket=sagemaker_session.default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training input uploaded to s3://sagemaker-us-east-2-786796469737/double_hard_debias/training-input-data\n"
     ]
    }
   ],
   "source": [
    "# training input location\n",
    "common_prefix = \"double_hard_debias\"\n",
    "training_input_prefix = common_prefix + \"/training-input-data\"\n",
    "TRAINING_WORKDIR = \"Input\"\n",
    "training_input = sagemaker_session.upload_data(TRAINING_WORKDIR, key_prefix=training_input_prefix)\n",
    "print(\"Training input uploaded to \" + training_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Execute DeBias Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that dataset is available in an accessible Amazon S3 bucket, we are ready to execute a DeBias model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_location = 's3://{}/double_hard_debias/{}'.format(bucket, 'output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For information on creating an `Estimator` object, see [documentation](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_instance_type='ml.m5.4xlarge'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-11 13:11:30 Starting - Starting the training job...\n",
      "2022-07-11 13:11:53 Starting - Preparing the instances for trainingProfilerReport-1657545089: InProgress\n",
      "......\n",
      "2022-07-11 13:12:59 Downloading - Downloading input data\n",
      "2022-07-11 13:12:59 Training - Training image download completed. Training in progress...\u001b[34mStarting the training.\u001b[0m\n",
      "\u001b[34mExtracting all the files now...\u001b[0m\n",
      "\u001b[34mFiles extraction from zip is Done!\u001b[0m\n",
      "\u001b[34mAll files found in uploaded zip file.\u001b[0m\n",
      "\u001b[34mText Pre-processing and Word Embeddings creating Algorithms initialized:\u001b[0m\n",
      "\u001b[34mSentences_list done\u001b[0m\n",
      "\u001b[34mcleaned_text_array done\u001b[0m\n",
      "\u001b[34mword_tokenized_array is generated\u001b[0m\n",
      "\u001b[34mStarted creating GloVe Word Embeddings!\u001b[0m\n",
      "\u001b[34mGloVe Model training done!\u001b[0m\n",
      "\u001b[34mGloVe Word Embeddings are created!\u001b[0m\n",
      "\u001b[34mStarted creating word2vec Word Embeddings!\u001b[0m\n",
      "\u001b[34mword2vec Model training done!\u001b[0m\n",
      "\u001b[34mGloVe Word Embeddings are created!\u001b[0m\n",
      "\u001b[34mDeBiasing Algorithm initialized:\u001b[0m\n",
      "\u001b[34mStarted creating GloVe & word2vec DeBiased Vectors!!\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/5477 [00:00<?, ?it/s]#015100%|██████████| 5477/5477 [00:00<00:00, 531222.90it/s]\u001b[0m\n",
      "\u001b[34msize of vocabulary: 5387\u001b[0m\n",
      "\u001b[34mpairs used in PCA:  8\u001b[0m\n",
      "\u001b[34mprecision 0.782\u001b[0m\n",
      "\u001b[34mpairs used in PCA:  8\u001b[0m\n",
      "\u001b[34mprecision 0.531\u001b[0m\n",
      "\u001b[34mpairs used in PCA:  8\u001b[0m\n",
      "\u001b[34mprecision 0.7695\u001b[0m\n",
      "\u001b[34mpairs used in PCA:  8\u001b[0m\n",
      "\u001b[34mprecision 0.7655000000000001\u001b[0m\n",
      "\u001b[34mpairs used in PCA:  8\u001b[0m\n",
      "\u001b[34mprecision 0.658\u001b[0m\n",
      "\u001b[34mpairs used in PCA:  8\u001b[0m\n",
      "\u001b[34mprecision 0.762\u001b[0m\n",
      "\u001b[34mpairs used in PCA:  8\u001b[0m\n",
      "\u001b[34mprecision 0.765\u001b[0m\n",
      "\u001b[34mpairs used in PCA:  8\u001b[0m\n",
      "\u001b[34mprecision 0.7625\u001b[0m\n",
      "\u001b[34mpairs used in PCA:  8\u001b[0m\n",
      "\u001b[34mprecision 0.7635\u001b[0m\n",
      "\u001b[34mpairs used in PCA:  8\u001b[0m\n",
      "\u001b[34mprecision 0.869\u001b[0m\n",
      "\u001b[34mpairs used in PCA:  8\u001b[0m\n",
      "\u001b[34mprecision 0.7605\u001b[0m\n",
      "\u001b[34mpairs used in PCA:  8\u001b[0m\n",
      "\u001b[34mprecision 0.7595\u001b[0m\n",
      "\u001b[34mpairs used in PCA:  8\u001b[0m\n",
      "\u001b[34mprecision 0.7575000000000001\u001b[0m\n",
      "\u001b[34mpairs used in PCA:  8\u001b[0m\n",
      "\u001b[34mprecision 0.7725\u001b[0m\n",
      "\u001b[34mpairs used in PCA:  8\u001b[0m\n",
      "\u001b[34mprecision 0.785\u001b[0m\n",
      "\u001b[34mpairs used in PCA:  8\u001b[0m\n",
      "\u001b[34mprecision 0.7735\u001b[0m\n",
      "\u001b[34mpairs used in PCA:  8\u001b[0m\n",
      "\u001b[34mprecision 0.7735\u001b[0m\n",
      "\u001b[34mpairs used in PCA:  8\u001b[0m\n",
      "\u001b[34mprecision 0.7625\u001b[0m\n",
      "\u001b[34mpairs used in PCA:  8\u001b[0m\n",
      "\u001b[34mprecision 0.7675\u001b[0m\n",
      "\u001b[34mpairs used in PCA:  8\u001b[0m\n",
      "\u001b[34mprecision 0.762\u001b[0m\n",
      "\u001b[34mDeBiasing Algorithm Applied!!\u001b[0m\n",
      "\u001b[34mExporting DeBiased GloVe Vectors!!\u001b[0m\n",
      "\u001b[34mOutput file named: GloVe_DeBiasedVectors.txt has been generated\u001b[0m\n",
      "\u001b[34mExporting DeBiased word2vec Vectors!!\u001b[0m\n",
      "\u001b[34mOutput file named: word2vec_DeBiasedVectors.txt is generated!\u001b[0m\n",
      "\u001b[34mTraining complete.\u001b[0m\n",
      "\n",
      "2022-07-11 13:17:54 Uploading - Uploading generated training model\n",
      "2022-07-11 13:18:34 Completed - Training job completed\n",
      "ProfilerReport-1657545089: NoIssuesFound\n",
      "Training seconds: 323\n",
      "Billable seconds: 323\n"
     ]
    }
   ],
   "source": [
    "#Create an estimator object for running a training job\n",
    "estimator = sage.algorithm.AlgorithmEstimator(\n",
    "    algorithm_arn=algo_arn,\n",
    "    base_job_name=\"debias-training\",\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type=training_instance_type,\n",
    "    input_mode=\"File\",\n",
    "    output_path=output_location,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    instance_count=1,\n",
    "    instance_type=training_instance_type\n",
    ")\n",
    "#Run the training job.\n",
    "estimator.fit({\"training\": training_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See this [blog-post](https://aws.amazon.com/blogs/machine-learning/easily-monitor-and-visualize-metrics-while-training-models-on-amazon-sagemaker/) for more information how to visualize metrics during the process. You can also open the training job from [Amazon SageMaker console](https://console.aws.amazon.com/sagemaker/home?#/jobs/) and monitor the metrics/logs in **Monitor** section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-2-786796469737/double_hard_debias/output'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#output is available on following path\n",
    "estimator.output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note: Inferencing is done within training pipeline. Real time inference endpoint/batch transform job is not required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Inspect the Output in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "parsed_url = urlparse(estimator.output_path)\n",
    "bucket_name = parsed_url.netloc\n",
    "file_key = parsed_url.path[1:]+'/'+estimator.latest_training_job.job_name+'/output/'+\"model.tar.gz\"\n",
    "\n",
    "s3_client = sagemaker_session.boto_session.client('s3')\n",
    "\n",
    "response = s3_client.get_object(Bucket = sagemaker_session.default_bucket(), Key = file_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucketFolder = estimator.output_path.rsplit('/')[3] +'/output/'+estimator.latest_training_job.job_name+'/output/'+\"model.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/boto3/compat.py:88: PythonDeprecationWarning: Boto3 will no longer support Python 3.6 starting May 30, 2022. To continue receiving service updates, bug fixes, and security updates please upgrade to Python 3.7 or later. More information can be found here: https://aws.amazon.com/blogs/developer/python-support-policy-updates-for-aws-sdks-and-tools/\n",
      "  warnings.warn(warning, PythonDeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output file loaded from bucket\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "s3_conn = boto3.client(\"s3\")\n",
    "bucket_name=bucket\n",
    "with open('output.tar.gz', 'wb') as f:\n",
    "    s3_conn.download_fileobj(bucket_name, bucketFolder, f)\n",
    "    print(\"Output file loaded from bucket\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tarfile.open('output.tar.gz') as file:\n",
    "    file.extractall('./output')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ZipFile('./output/outputDeBiasedVectors.zip', \"r\") as output_zip:\n",
    "    with io.TextIOWrapper(output_zip.open(\"GloVe_DeBiasedVectors.txt\"), encoding=\"utf-8\") as f:\n",
    "        r = f.read()\n",
    "    pprint(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ZipFile('./output/outputDeBiasedVectors.zip', \"r\") as output_zip:\n",
    "    with io.TextIOWrapper(output_zip.open(\"word2vec_DeBiasedVectors.txt\"), encoding=\"utf-8\") as f:\n",
    "        r = f.read()\n",
    "    pprint(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Clean-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unsubscribe to the listing (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to unsubscribe to the algorithm, follow these steps. Before you cancel the subscription, ensure that you do not have any [deployable model](https://console.aws.amazon.com/sagemaker/home#/models) created from the model package or using the algorithm. Note - You can find this information by looking at the container name associated with the model. \n",
    "\n",
    "**Steps to unsubscribe to product from AWS Marketplace**:\n",
    "1. Navigate to __Machine Learning__ tab on [__Your Software subscriptions page__](https://aws.amazon.com/marketplace/ai/library?productType=ml&ref_=mlmp_gitdemo_indust)\n",
    "2. Locate the listing that you want to cancel the subscription for, and then choose __Cancel Subscription__  to cancel the subscription.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
